{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "from keras.layers import Dense, Input, Lambda, Conv2D \n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class A2C:\n",
    "    \n",
    "    def __init__(self, lr_a, lr_c, gamma, epsilon, obs_dims, action_size, batch_size=1):\n",
    "        \n",
    "        self.logging = False\n",
    "        \n",
    "        # hiperparams\n",
    "        self.lr_a = lr_a                         #learning rate actor\n",
    "        self.lr_c = lr_c                         #learning rate critic\n",
    "        self.epsilon = epsilon                   #exploration rate ????????\n",
    "        self.gamma = gamma                       #discount factor\n",
    "        self.batch_size = batch_size             #number of frames the agents gets after every action, or Exp. Replay\n",
    "        \n",
    "        # dimensions\n",
    "        self.obs_dims = obs_dims                 #dimensions of the input\n",
    "        self.obs_size = np.prod(obs_dims)        #size of the input\n",
    "        self.action_size = action_size           #number of action\n",
    "        print(\"NETWORK INIT, obsdim, obsize, actsize: \", self.obs_dims, self.obs_size, self.action_size)\n",
    "        self.value_size = 1                      #output dim of the cirtic\n",
    "        \n",
    "        # models\n",
    "        self.actor, self.critic = self.build_models()\n",
    "        \n",
    "        #self.actor = self.build_actor()\n",
    "        #self.critic = self.build_critic()\n",
    "        \n",
    "        self.actor_optimizer, self.critic_optimizer = self.build_actor_optimizer(), self.build_critic_optimizer()\n",
    "        \n",
    "        # serialization\n",
    "        self.load_checkpoint = False\n",
    "        self.save_checkpoint = False\n",
    "        self.save_destination = \"/Users/daddy/Desktop/projekt/\" + str(self.epsilon) + \"agent/\"\n",
    "        \n",
    "        if self.load_checkpoint:\n",
    "            self.actor.load_weights(self.save_destination + \"weights_actor.h5\")\n",
    "            self.critic.load_weights(self.save_destination + \"weights_critic.h5\")\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def build_actor(self):\n",
    "        \n",
    "        # approximate policy and value using Neural Network\n",
    "        # actor: state is input and probability of each action is output of model\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(24, input_dim=self.obs_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        #actor.compile(loss='categorical_crossentropy',   #H(p, q)  = sum( p_i * log(q_i)) ,  \n",
    "        #optimizer=Adam(lr=self.lr_a))                    # actor loss = sum( Adv. * log(P(a∣s)))\n",
    "        return actor\n",
    "\n",
    "    \n",
    "    def build_critic(self):\n",
    "        # critic: state is input and value of state is output of model\n",
    "\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(24, input_dim=self.obs_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.lr_c))\n",
    "        \n",
    "        return critic\n",
    "\n",
    "\n",
    "        \n",
    "    def build_models(self):\n",
    "        \n",
    "        observation = Input(batch_shape=(None, self.obs_size))\n",
    "        \n",
    "        # Shared Stream\n",
    "        #l1_shared = Dense(24,  activation='sigmoid', kernel_initializer='he_uniform')(observation)\n",
    "        #l2_shared = Dense(8, activation='sigmoid', kernel_initializer='he_uniform')(l1_shared)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Actor Stream\n",
    "        #l3_actor = Dense(8, activation='sigmoid', kernel_initializer='he_uniform')(l1_shared)\n",
    "        #actor_output = Dense(self.action_size, activation='softmax', kernel_initializer='he_uniform')(l3_actor)\n",
    "\n",
    "        # Critic Stream\n",
    "        #l3_critic= Dense(8, activation='sigmoid', kernel_initializer='he_uniform')(l1_shared)\n",
    "        #critic_output = Dense(self.value_size, activation='linear', kernel_initializer='he_uniform')(l3_critic)\n",
    "        \n",
    "        #model = Model(input=state_input, output=[actor, critic])\n",
    "        \n",
    "        \n",
    "        observation = Input(shape=self.state_size)\n",
    "        conv1 = Conv2D(16, (8, 8), strides=(4, 4), activation='relu')(observation)\n",
    "        conv2 = Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv1)\n",
    "        conv3 = Flatten()(conv2)\n",
    "        fc1 = Dense(256, activation='relu')(conv)\n",
    "        policy = Dense(self.action_size, activation='softmax')(fc)\n",
    "        value = Dense(1, activation='linear')(fc)\n",
    "\n",
    "        actor = Model(inputs=observation, outputs=policy)\n",
    "        critic = Model(inputs=observtion, outputs=value)\n",
    "\n",
    "        actor.summary()\n",
    "        critic.summary()\n",
    "\n",
    "        return actor, critic\n",
    "\n",
    "        optim_a = Adam(lr=self.lr_a)\n",
    "        #optim_c = Adam(lr=self.lr_c)\n",
    "\n",
    "        # the loss function of policy network is : log(action_prob) * advantages , which is form of cross entropy.\n",
    "        actor.compile(loss='categorical_crossentropy', optimizer=optim_a)\n",
    "        #critic.compile(loss='mse', optimizer=optim_c)\n",
    "\n",
    "        actor.summary()\n",
    "        critic.summary()\n",
    "        \n",
    "        return actor, critic\n",
    "\n",
    "    def build_actor_optimizer(self):\n",
    "        \n",
    "        action = K.placeholder(shape=[None, self.action_size])\n",
    "        advantages = K.placeholder(shape=[None, ])\n",
    "\n",
    "        policy = self.actor.output\n",
    "\n",
    "        good_prob = K.sum(action * policy, axis=1)\n",
    "        eligibility = K.log(good_prob + 1e-10) * advantages\n",
    "        actor_loss = -K.sum(eligibility)\n",
    "\n",
    "        entropy = K.sum(policy * K.log(policy + 1e-10), axis=1)\n",
    "        entropy = K.sum(entropy)\n",
    "\n",
    "        loss = actor_loss + 0.01*entropy\n",
    "        optimizer = RMSprop(lr=self.actor_lr, rho=0.99, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], loss)\n",
    "        train = K.function([self.actor.input, action, advantages], [loss], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    def build_critic_optimizer(self):\n",
    "        \n",
    "        target = K.placeholder(shape=(None, 1))\n",
    "\n",
    "        value = self.critic.output\n",
    "\n",
    "        loss_c = K.mean(K.square(target - value)) #mse\n",
    "        #loss_c = K.mean(K.square(target))\n",
    "\n",
    "        optimizer = Adam(lr=self.lr_c)\n",
    "        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss_c)\n",
    "\n",
    "        train = K.function([self.critic.input, target], [], updates=updates)\n",
    "\n",
    "        return train\n",
    "        \n",
    "    \n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "\n",
    "        # update policy network every episode\n",
    "        \n",
    "        target = np.zeros((1, self.value_size))          #target value to train the critic\n",
    "        advantages = np.zeros((1, self.action_size))     #advantage of the action, to train the actor\n",
    "        \n",
    "        state = np.reshape(state, (1, self.obs_size))\n",
    "        next_state = np.reshape(next_state, [1, self.obs_size])\n",
    "        \n",
    "        value = self.critic.predict(state)[0]            #value of current state\n",
    "        next_value = self.critic.predict(next_state)[0]  #value of the next state\n",
    "        \n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.gamma * (next_value) - value   # TD learning:\n",
    "            target[0][0] = reward + self.gamma * next_value  # target = Rt + gamma*V(St+1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.logging:\n",
    "            \n",
    "            print(\"state \", state)\n",
    "            print(\"values: \", value, next_value)    \n",
    "            print(\"adv: \", advantages)\n",
    "            print(\"target :\", target)\n",
    "            print(\"---------------------\")\n",
    "\n",
    "        # or /w the optimizers\n",
    "        #self.actor_optimizer([state, action, advantages])\n",
    "        self.critic_optimizer([state, target])\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        #self.critic.fit(state, target, epochs=1, verbose=0)\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \n",
    "        policy = self.actor.predict(np.reshape(state, [1, self.obs_size])).flatten()      #array with probs. of taking every action\n",
    "        action = np.random.choice(self.action_size, 1, p=policy)[0]     #sampling from policys distribution\n",
    "        if self.logging:             \n",
    "            print(\"policy:\", policy, \" --> action:\", action)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def save_weights(self):\n",
    "        if self.save_checkpoint:\n",
    "            self.actor.save_weights(self.save_destination + \"weights_actor.h5\")\n",
    "            self.critic.save_weights(self.save_destination + \"weights_critic.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, env_name , lr_a, lr_c, gamma, epsilon):\n",
    "\n",
    "        self.env = gym.make(env_name)\n",
    "        self.scores = []\n",
    "\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.obs_dims = self.env.observation_space.shape[0]\n",
    "        self.render = False\n",
    "\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "\n",
    "        self.networks = A2C(self.lr_a, self.lr_c, self.gamma, self.epsilon, self.obs_dims, self.action_size)\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.networks.get_action(state)\n",
    "\n",
    "    def train(self, EPISODES, plot=False):\n",
    "        \n",
    "        \n",
    "        for e in range(EPISODES):\n",
    "\n",
    "            done = False\n",
    "            score = 0\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            while not done: \n",
    "\n",
    "                if self.render == True:\n",
    "                    self.env.render()\n",
    "\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                #reward = reward if not done else -100   #or this is the last timestep in the episode\n",
    "                self.networks.update(state, action, reward, next_state, done)\n",
    "\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                # every episode, plot the play time\n",
    "                    self.scores.append(score)\n",
    "                    print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                if e%500:\n",
    "                    self.networks.save_weights()\n",
    "\n",
    "\n",
    "        self.networks.save_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-04 17:11:29,380] Making new env: Breakout-v3\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'atari_py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dc7009f7fd87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Breakout-v3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-1e4353fa25e3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_name, lr_a, lr_c, gamma, epsilon)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlr_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestep_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vnc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_limit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeLimit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempting to make deprecated env {}. (HINT: is there a newer registered version of this env?)'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mentry_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEntryPoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, require, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2409\u001b[0m         \u001b[0mResolve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mits\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2410\u001b[0m         \"\"\"\n\u001b[0;32m-> 2411\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__name__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2412\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2413\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gym/envs/atari/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAtariEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)"
     ]
    }
   ],
   "source": [
    "a = Agent(\"Breakout-v3\", 0.01, 0.01, 0.99, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_MA(scores, ma=10):\n",
    "    #plotting\n",
    "\n",
    "    x, y = [], []\n",
    "    maxes  = []\n",
    "    temp = []\n",
    "    moving_avg =[]\n",
    "    m_x = []\n",
    "\n",
    "    scores = n.scores\n",
    "    for i in range(len(scores)):\n",
    "        temp.append(scores[i])\n",
    "        m_x.append(i+1)\n",
    "        if i % ma == 0:\n",
    "        #    y.append(np.mean(temp))\n",
    "            maxes.append(max(temp))\n",
    "            temp = []\n",
    "            x.append(i+1)\n",
    "        if i < ma:\n",
    "            moving_avg.append(scores[i])\n",
    "        else:\n",
    "            moving_avg.append(np.mean(scores[i-ma:i]))\n",
    "\n",
    "\n",
    "\n",
    "    #y.append(scores[len(scores)-1])\n",
    "    #x.append(i+1)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    plt.plot(m_x, moving_avg)\n",
    "    plt.scatter(x, maxes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
